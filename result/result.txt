C:\Users\wcf\AppData\Local\Programs\Python\Python37\python.exe C:/Users/wcf/PycharmProjects/pokeman_selector/main.py
Epoch 0/19:
loss: 2.5000531673431396
loss: 16.122394561767578
loss: 10.938665390014648
loss: 12.637918472290039
loss: 10.638967514038086
loss: 5.965609550476074
loss: 4.588322639465332
loss: 5.162423133850098
loss: 7.069740295410156
loss: 5.810865879058838
loss: 5.320096969604492
loss: 3.848828077316284
loss: 2.759528875350952
loss: 2.9046947956085205
loss: 2.5446276664733887
loss: 2.6390607357025146
loss: 2.38313364982605
loss: 2.613309383392334
loss: 2.469334125518799
loss: 2.565082550048828
loss: 2.4925293922424316
loss: 2.7631096839904785
loss: 2.484745502471924
loss: 2.498030185699463
loss: 2.6112728118896484
loss: 2.5036988258361816
loss: 2.5182247161865234
loss: 2.355952501296997
loss: 2.4171481132507324
loss: 2.5653693675994873
loss: 2.386162519454956
loss: 2.5949559211730957
loss: 2.291891098022461
loss: 2.1799798011779785
loss: 2.4914908409118652
loss: 2.1883325576782227
loss: 2.0596725940704346
loss: 2.2074453830718994
loss: 2.676818609237671
loss: 2.149413824081421
loss: 2.4143824577331543
loss: 2.347543239593506
loss: 2.5394656658172607
loss: 2.3685317039489746
loss: 2.527764081954956
loss: 2.3072991371154785
train_acc: 0.1956043956043956
test_acc: 0.1568627450980392

Epoch 1/19:
loss: 2.7399260997772217
loss: 2.442791223526001
loss: 1.8875049352645874
loss: 2.271885395050049
loss: 2.348752498626709
loss: 2.0356709957122803
loss: 2.5718512535095215
loss: 1.9115879535675049
loss: 1.714477300643921
loss: 2.1554312705993652
loss: 2.3248791694641113
loss: 2.2539689540863037
loss: 1.9530246257781982
loss: 1.977489709854126
loss: 2.2597644329071045
loss: 1.7209532260894775
loss: 2.2796263694763184
loss: 2.508972644805908
loss: 2.187080144882202
loss: 2.1608452796936035
loss: 2.222754716873169
loss: 1.8627071380615234
loss: 1.8286895751953125
loss: 2.082465887069702
loss: 2.1767046451568604
loss: 1.9420379400253296
loss: 1.7335389852523804
loss: 1.4626028537750244
loss: 1.87680983543396
loss: 2.376117706298828
loss: 1.675082802772522
loss: 1.637792944908142
loss: 1.6494672298431396
loss: 1.0809383392333984
loss: 1.9777677059173584
loss: 2.4126932621002197
loss: 1.5908358097076416
loss: 2.0265727043151855
loss: 1.8837711811065674
loss: 1.9634453058242798
loss: 2.5176196098327637
loss: 2.3976755142211914
loss: 2.023428440093994
loss: 2.2187397480010986
loss: 2.089369297027588
loss: 2.080615282058716
train_acc: 0.3648351648351648
test_acc: 0.32941176470588235

Epoch 2/19:
loss: 1.5427062511444092
loss: 1.5393311977386475
loss: 2.0029444694519043
loss: 1.8862653970718384
loss: 1.3170182704925537
loss: 1.3899110555648804
loss: 1.675276756286621
loss: 1.1701066493988037
loss: 1.2477995157241821
loss: 2.152705192565918
loss: 2.167168617248535
loss: 1.7131935358047485
loss: 1.7828887701034546
loss: 0.976677417755127
loss: 1.3286949396133423
loss: 1.5318726301193237
loss: 1.4179003238677979
loss: 1.5137007236480713
loss: 1.1916310787200928
loss: 1.373094916343689
loss: 1.0350306034088135
loss: 1.7358500957489014
loss: 1.8668394088745117
loss: 1.0380131006240845
loss: 1.8663692474365234
loss: 1.2796283960342407
loss: 2.6138715744018555
loss: 1.2509040832519531
loss: 1.8036285638809204
loss: 1.1338104009628296
loss: 1.165926218032837
loss: 1.2296135425567627
loss: 2.0203518867492676
loss: 2.331066846847534
loss: 1.663971185684204
loss: 1.3887386322021484
loss: 1.700061559677124
loss: 1.355231761932373
loss: 1.4831100702285767
loss: 1.7445653676986694
loss: 0.8770381808280945
loss: 0.8471258878707886
loss: 1.7993484735488892
loss: 1.1926770210266113
loss: 1.2445721626281738
loss: 1.0842313766479492
train_acc: 0.4989010989010989
test_acc: 0.4627450980392157

Epoch 3/19:
loss: 1.1151596307754517
loss: 1.085407018661499
loss: 1.8901078701019287
loss: 1.5141507387161255
loss: 1.0701398849487305
loss: 2.0453436374664307
loss: 1.552749514579773
loss: 1.3626863956451416
loss: 1.3322460651397705
loss: 0.9321768879890442
loss: 1.540592908859253
loss: 1.4901199340820312
loss: 0.7657287120819092
loss: 0.8284412622451782
loss: 1.1608803272247314
loss: 1.2507935762405396
loss: 1.0325658321380615
loss: 1.093929648399353
loss: 1.116258144378662
loss: 1.1485602855682373
loss: 2.0893056392669678
loss: 0.43914610147476196
loss: 1.7555866241455078
loss: 1.2061374187469482
loss: 1.1534740924835205
loss: 0.727862536907196
loss: 1.0969855785369873
loss: 1.076271653175354
loss: 0.4458061158657074
loss: 0.9794977903366089
loss: 0.9178360104560852
loss: 1.6060092449188232
loss: 1.3187702894210815
loss: 1.3951735496520996
loss: 0.9376264810562134
loss: 1.2594399452209473
loss: 0.7874116897583008
loss: 1.0797159671783447
loss: 1.1845656633377075
loss: 0.4159296452999115
loss: 0.5581627488136292
loss: 1.5792675018310547
loss: 0.9301549792289734
loss: 0.8007093667984009
loss: 1.0751310586929321
loss: 0.9820486307144165
train_acc: 0.654945054945055
test_acc: 0.6274509803921569

Epoch 4/19:
loss: 1.2727525234222412
loss: 0.453883558511734
loss: 0.24653485417366028
loss: 0.7582607269287109
loss: 1.1373779773712158
loss: 0.8368984460830688
loss: 1.236637830734253
loss: 0.7152479887008667
loss: 0.8794581294059753
loss: 1.6001472473144531
loss: 0.9810668230056763
loss: 0.9921606183052063
loss: 1.6140289306640625
loss: 1.043393611907959
loss: 0.9291410446166992
loss: 0.6805564165115356
loss: 0.8080394864082336
loss: 0.922873318195343
loss: 0.8993978500366211
loss: 0.5568959712982178
loss: 0.6816250681877136
loss: 1.5015207529067993
loss: 0.3039839267730713
loss: 0.44554609060287476
loss: 0.8528372049331665
loss: 1.7476673126220703
loss: 1.1359803676605225
loss: 0.8451641201972961
loss: 0.5100023746490479
loss: 0.9528848528862
loss: 1.2051384449005127
loss: 0.7729658484458923
loss: 2.0132853984832764
loss: 1.1263905763626099
loss: 0.8994691967964172
loss: 0.7872589230537415
loss: 1.603809118270874
loss: 0.9753826856613159
loss: 0.6584321856498718
loss: 1.5332210063934326
loss: 0.6355751752853394
loss: 1.128787875175476
loss: 1.4469783306121826
loss: 1.9574425220489502
loss: 1.4438310861587524
loss: 1.495343565940857
train_acc: 0.621978021978022
test_acc: 0.5686274509803921

Epoch 5/19:
loss: 1.0010559558868408
loss: 0.3449975252151489
loss: 1.0085232257843018
loss: 0.9174053072929382
loss: 0.9885179400444031
loss: 1.2112897634506226
loss: 1.7403976917266846
loss: 0.8523724675178528
loss: 0.6279522180557251
loss: 0.5809601545333862
loss: 1.0684807300567627
loss: 0.9194326400756836
loss: 2.008061170578003
loss: 2.0023837089538574
loss: 0.7895963788032532
loss: 1.4770848751068115
loss: 0.9024945497512817
loss: 0.7396125793457031
loss: 0.4229621887207031
loss: 0.45159491896629333
loss: 1.0731079578399658
loss: 0.38054558634757996
loss: 0.6991602182388306
loss: 1.09758722782135
loss: 1.9954807758331299
loss: 1.0813184976577759
loss: 1.1286784410476685
loss: 1.0704792737960815
loss: 0.7457038164138794
loss: 1.1256078481674194
loss: 0.4863927960395813
loss: 0.5203158855438232
loss: 0.7588302493095398
loss: 1.4539058208465576
loss: 0.8343756794929504
loss: 1.379256248474121
loss: 1.9556877613067627
loss: 0.8155673146247864
loss: 0.5857330560684204
loss: 0.4906352162361145
loss: 1.1598825454711914
loss: 0.819463849067688
loss: 1.0383105278015137
loss: 1.1771020889282227
loss: 1.1489489078521729
loss: 1.5307629108428955
train_acc: 0.7164835164835165
test_acc: 0.7058823529411765

Epoch 6/19:
loss: 0.46272438764572144
loss: 0.9998359680175781
loss: 0.8025892972946167
loss: 0.8641694188117981
loss: 0.8659117817878723
loss: 0.638454258441925
loss: 1.1467642784118652
loss: 1.2914259433746338
loss: 0.3956535756587982
loss: 0.970309853553772
loss: 0.4705283045768738
loss: 1.2123744487762451
loss: 0.5305361747741699
loss: 0.9775153398513794
loss: 1.0828323364257812
loss: 0.9001947641372681
loss: 0.40081310272216797
loss: 0.7976058721542358
loss: 0.9617089033126831
loss: 0.6990499496459961
loss: 0.8637551069259644
loss: 1.6717790365219116
loss: 0.9817087054252625
loss: 1.268336534500122
loss: 1.0172417163848877
loss: 1.119936227798462
loss: 1.0576918125152588
loss: 1.1680448055267334
loss: 0.867640495300293
loss: 0.9016245007514954
loss: 0.7558684349060059
loss: 1.6042789220809937
loss: 1.4774481058120728
loss: 0.8540412187576294
loss: 0.7354946732521057
loss: 0.751295804977417
loss: 0.6757365465164185
loss: 0.7495160698890686
loss: 1.873578429222107
loss: 0.9575681686401367
loss: 0.5904656052589417
loss: 1.0011931657791138
loss: 1.24933922290802
loss: 0.3936108648777008
loss: 0.9126622080802917
loss: 0.9014945030212402
train_acc: 0.7252747252747253
test_acc: 0.6784313725490196

Epoch 7/19:
loss: 0.6146872639656067
loss: 0.5731507539749146
loss: 0.9543622136116028
loss: 0.513410210609436
loss: 0.8615299463272095
loss: 1.0433443784713745
loss: 0.5506210327148438
loss: 1.2143409252166748
loss: 0.8936138153076172
loss: 0.6138919591903687
loss: 0.638538122177124
loss: 0.42213669419288635
loss: 1.2085175514221191
loss: 0.8794364929199219
loss: 0.634813666343689
loss: 1.2254496812820435
loss: 1.3370786905288696
loss: 0.6067022681236267
loss: 0.9307200312614441
loss: 0.8257490992546082
loss: 0.27046334743499756
loss: 0.58120197057724
loss: 0.693305492401123
loss: 0.8259121179580688
loss: 0.6440773606300354
loss: 0.5182997584342957
loss: 0.6625946760177612
loss: 0.9677797555923462
loss: 0.7477449178695679
loss: 0.6013356447219849
loss: 0.698557436466217
loss: 1.5561397075653076
loss: 1.4289177656173706
loss: 1.5277678966522217
loss: 0.8662471771240234
loss: 1.4533997774124146
loss: 0.6086688041687012
loss: 0.9031976461410522
loss: 0.1849357634782791
loss: 0.5921674370765686
loss: 0.6324599385261536
loss: 0.9329452514648438
loss: 0.6976150274276733
loss: 0.6343134045600891
loss: 0.8880499005317688
loss: 0.6490024328231812
train_acc: 0.7670329670329671
test_acc: 0.7215686274509804

Epoch 8/19:
loss: 1.1468430757522583
loss: 1.2123329639434814
loss: 0.9142448306083679
loss: 0.9495880007743835
loss: 0.8289651870727539
loss: 0.8279925584793091
loss: 1.1171720027923584
loss: 0.8435896635055542
loss: 1.1109845638275146
loss: 0.4966356158256531
loss: 0.35055315494537354
loss: 0.7078479528427124
loss: 1.0835789442062378
loss: 1.3448472023010254
loss: 0.4639013409614563
loss: 1.0762407779693604
loss: 0.6997026205062866
loss: 1.170274019241333
loss: 0.38185110688209534
loss: 0.8110538721084595
loss: 0.9144662618637085
loss: 1.0966129302978516
loss: 0.6471285223960876
loss: 0.9090743064880371
loss: 0.565362274646759
loss: 0.4544604420661926
loss: 1.056030035018921
loss: 0.30932778120040894
loss: 0.6171513199806213
loss: 1.7505229711532593
loss: 0.18278494477272034
loss: 0.8174320459365845
loss: 0.29759272933006287
loss: 2.8135313987731934
loss: 1.8179512023925781
loss: 0.8339076042175293
loss: 0.4106450080871582
loss: 0.48414069414138794
loss: 1.7274110317230225
loss: 0.6350473165512085
loss: 2.6907026767730713
loss: 0.8730918765068054
loss: 0.8486631512641907
loss: 0.4852002263069153
loss: 1.3167352676391602
loss: 0.3588274121284485
train_acc: 0.6571428571428571
test_acc: 0.6352941176470588

Epoch 9/19:
loss: 1.094345211982727
loss: 1.0685852766036987
loss: 0.7238065004348755
loss: 1.2589036226272583
loss: 1.1126357316970825
loss: 0.7599145770072937
loss: 0.7010837197303772
loss: 0.46609678864479065
loss: 1.0450040102005005
loss: 0.7422752380371094
loss: 1.418034315109253
loss: 0.7318083047866821
loss: 0.574485182762146
loss: 0.4204859733581543
loss: 1.2415376901626587
loss: 2.3718249797821045
loss: 1.6249759197235107
loss: 0.5411043167114258
loss: 0.7691162824630737
loss: 0.8455332517623901
loss: 0.966568648815155
loss: 0.48973292112350464
loss: 1.3125394582748413
loss: 0.9361572265625
loss: 1.065281867980957
loss: 0.7412323951721191
loss: 0.796882152557373
loss: 1.7157485485076904
loss: 1.7401092052459717
loss: 0.4629160761833191
loss: 1.9755455255508423
loss: 0.7628685832023621
loss: 1.3499265909194946
loss: 0.937227725982666
loss: 0.9407427906990051
loss: 0.7730962038040161
loss: 0.4034239649772644
loss: 1.2550504207611084
loss: 0.8363686800003052
loss: 0.949489414691925
loss: 0.6276836395263672
loss: 0.38667961955070496
loss: 1.08296799659729
loss: 0.8001812100410461
loss: 0.944993793964386
loss: 0.6557102203369141
train_acc: 0.7714285714285715
test_acc: 0.7137254901960784

Epoch 10/19:
loss: 1.166932225227356
loss: 0.48608413338661194
loss: 0.12386069446802139
loss: 0.8188868761062622
loss: 0.9193283319473267
loss: 1.2172633409500122
loss: 0.5699470639228821
loss: 0.5166054964065552
loss: 1.076054334640503
loss: 0.4728248715400696
loss: 1.251689076423645
loss: 1.2325398921966553
loss: 0.7971709966659546
loss: 0.31579065322875977
loss: 1.5841232538223267
loss: 0.3622278571128845
loss: 0.6531509160995483
loss: 1.1776777505874634
loss: 1.0091131925582886
loss: 1.0671284198760986
loss: 0.8490839004516602
loss: 1.0762821435928345
loss: 0.2041873037815094
loss: 0.9762639999389648
loss: 0.4715883135795593
loss: 0.6238546371459961
loss: 2.1294705867767334
loss: 1.0461076498031616
loss: 0.7985461354255676
loss: 1.164339303970337
loss: 0.8672013282775879
loss: 0.370840847492218
loss: 1.298647165298462
loss: 1.1981785297393799
loss: 0.2698262631893158
loss: 0.9260317087173462
loss: 0.25641578435897827
loss: 0.626825749874115
loss: 0.10638026893138885
loss: 0.5300289988517761
loss: 0.32366642355918884
loss: 0.6029307842254639
loss: 0.3294164836406708
loss: 0.7559480667114258
loss: 0.839404284954071
loss: 0.3239261507987976
train_acc: 0.7736263736263737
test_acc: 0.7058823529411765

Epoch 11/19:
loss: 0.7022355198860168
loss: 0.4656294882297516
loss: 0.6841374635696411
loss: 0.3671633005142212
loss: 0.32937073707580566
loss: 0.8656944036483765
loss: 0.7403815984725952
loss: 0.7452616691589355
loss: 0.6932889223098755
loss: 0.39881810545921326
loss: 0.35784080624580383
loss: 0.5471101999282837
loss: 0.6418399810791016
loss: 0.6292003989219666
loss: 0.4354259967803955
loss: 1.2588199377059937
loss: 0.31786540150642395
loss: 1.0294734239578247
loss: 0.9560707211494446
loss: 1.1150201559066772
loss: 0.8434158563613892
loss: 0.4983891546726227
loss: 0.5345209240913391
loss: 0.0426175594329834
loss: 0.7542027831077576
loss: 1.3590513467788696
loss: 0.5717059373855591
loss: 0.2568082809448242
loss: 0.812149703502655
loss: 0.4405137002468109
loss: 0.695468008518219
loss: 1.1381484270095825
loss: 1.823366403579712
loss: 1.643800973892212
loss: 0.6138049960136414
loss: 0.7911108732223511
loss: 1.0744907855987549
loss: 0.4211309552192688
loss: 1.204911231994629
loss: 0.4218824505805969
loss: 0.9266176223754883
loss: 0.7648792266845703
loss: 0.4890422821044922
loss: 0.5481545329093933
loss: 1.0004225969314575
loss: 0.8470937609672546
train_acc: 0.7538461538461538
test_acc: 0.7098039215686275

Epoch 12/19:
loss: 0.4962386190891266
loss: 0.8168227076530457
loss: 0.6343957781791687
loss: 0.23316100239753723
loss: 0.22361412644386292
loss: 0.6856312155723572
loss: 0.3750946521759033
loss: 0.5726948976516724
loss: 1.131922721862793
loss: 0.5400999188423157
loss: 0.7221745252609253
loss: 0.6349889636039734
loss: 1.0043262243270874
loss: 0.6059030294418335
loss: 0.10414175689220428
loss: 0.7349074482917786
loss: 0.6099694967269897
loss: 0.5094830393791199
loss: 0.6035473942756653
loss: 0.5300286412239075
loss: 0.45840930938720703
loss: 0.6727026104927063
loss: 0.9870285987854004
loss: 0.7160839438438416
loss: 0.5059288144111633
loss: 0.48838967084884644
loss: 0.630896270275116
loss: 1.3887593746185303
loss: 0.9043133854866028
loss: 0.5021049976348877
loss: 0.39727848768234253
loss: 0.3034951388835907
loss: 0.28562402725219727
loss: 0.4235590100288391
loss: 0.290238618850708
loss: 1.672015905380249
loss: 0.3662213385105133
loss: 0.8509007692337036
loss: 0.3213197588920593
loss: 0.8281154632568359
loss: 0.48960790038108826
loss: 0.7663232088088989
loss: 0.6448397636413574
loss: 0.9363314509391785
loss: 0.6966612935066223
loss: 0.7438820600509644
train_acc: 0.8153846153846154
test_acc: 0.7607843137254902

Epoch 13/19:
loss: 0.6103023290634155
loss: 0.6012101173400879
loss: 0.32727622985839844
loss: 0.3778969347476959
loss: 0.36220353841781616
loss: 0.7453838586807251
loss: 0.4318710267543793
loss: 0.5730952024459839
loss: 0.5976276397705078
loss: 0.25670623779296875
loss: 0.43003708124160767
loss: 0.8869524002075195
loss: 0.21531836688518524
loss: 0.5385589003562927
loss: 0.5426212549209595
loss: 1.6172764301300049
loss: 0.16260595619678497
loss: 0.7318187952041626
loss: 0.8628441691398621
loss: 0.6824682950973511
loss: 0.32645297050476074
loss: 0.6424118876457214
loss: 0.16449889540672302
loss: 0.5852838158607483
loss: 0.6484089493751526
loss: 0.27339428663253784
loss: 0.1528792828321457
loss: 0.8740189671516418
loss: 0.6837317943572998
loss: 0.6666573286056519
loss: 1.6470692157745361
loss: 1.1459522247314453
loss: 0.6402488946914673
loss: 0.4194198548793793
loss: 0.4266524314880371
loss: 0.5011186599731445
loss: 0.2678045630455017
loss: 0.45924410223960876
loss: 0.319074422121048
loss: 0.5029409527778625
loss: 0.4309155344963074
loss: 0.5424591302871704
loss: 0.5428927540779114
loss: 0.1705012321472168
loss: 0.08696947246789932
loss: 0.13819608092308044
train_acc: 0.8769230769230769
test_acc: 0.8117647058823529

Epoch 14/19:
loss: 0.10844063758850098
loss: 0.7154275178909302
loss: 0.3114747405052185
loss: 0.41153860092163086
loss: 0.3003050684928894
loss: 0.3601682782173157
loss: 0.4573230743408203
loss: 0.4867883324623108
loss: 0.11338584125041962
loss: 0.8951940536499023
loss: 0.45580077171325684
loss: 0.2794979214668274
loss: 0.3658391535282135
loss: 0.19737477600574493
loss: 0.5481727719306946
loss: 0.9417988061904907
loss: 0.546337902545929
loss: 0.6561876535415649
loss: 0.5222789645195007
loss: 0.685406506061554
loss: 0.4554471969604492
loss: 0.2783014178276062
loss: 0.530903697013855
loss: 0.6273714303970337
loss: 0.38953670859336853
loss: 0.17176523804664612
loss: 0.3562375009059906
loss: 0.8303176760673523
loss: 0.5217939615249634
loss: 1.1962215900421143
loss: 0.3105980157852173
loss: 0.3540217876434326
loss: 0.5362904071807861
loss: 0.6804659366607666
loss: 0.43913382291793823
loss: 0.5333720445632935
loss: 0.910779595375061
loss: 0.2209794521331787
loss: 0.09044747054576874
loss: 0.5870050191879272
loss: 0.5417963862419128
loss: 0.06511974334716797
loss: 0.8389180898666382
loss: 0.43266886472702026
loss: 0.42383337020874023
loss: 0.4734341502189636
train_acc: 0.8197802197802198
test_acc: 0.7568627450980392

Epoch 15/19:
loss: 0.26366010308265686
loss: 0.1939094513654709
loss: 0.17908744513988495
loss: 0.9682564735412598
loss: 0.40648898482322693
loss: 0.4949073791503906
loss: 0.4722779393196106
loss: 0.9311026334762573
loss: 0.525274932384491
loss: 0.395221084356308
loss: 0.5243845582008362
loss: 0.31312718987464905
loss: 0.39892715215682983
loss: 0.29253363609313965
loss: 0.37063702940940857
loss: 0.2754671573638916
loss: 0.1886463463306427
loss: 0.146937757730484
loss: 0.4410591125488281
loss: 0.127947136759758
loss: 0.171100452542305
loss: 0.2589893639087677
loss: 0.558357834815979
loss: 0.32219988107681274
loss: 0.460078626871109
loss: 0.43563294410705566
loss: 0.07266898453235626
loss: 0.20306257903575897
loss: 0.3649462163448334
loss: 0.4687003493309021
loss: 0.3058149218559265
loss: 0.2950703203678131
loss: 0.4176185727119446
loss: 0.3159123957157135
loss: 0.7825066447257996
loss: 0.36709827184677124
loss: 0.10668887943029404
loss: 0.20559954643249512
loss: 0.22802619636058807
loss: 0.4077027440071106
loss: 0.061566732823848724
loss: 0.4351631999015808
loss: 0.06312570720911026
loss: 0.19155769050121307
loss: 0.32155799865722656
loss: 0.1268741637468338
train_acc: 0.8747252747252747
test_acc: 0.796078431372549

Epoch 16/19:
loss: 0.17801889777183533
loss: 0.18401174247264862
loss: 0.16367760300636292
loss: 0.03892211988568306
loss: 0.29537874460220337
loss: 0.2011449784040451
loss: 0.7889668345451355
loss: 0.6961526870727539
loss: 0.2587623596191406
loss: 0.19376620650291443
loss: 0.67302405834198
loss: 0.22877392172813416
loss: 0.48173436522483826
loss: 0.050000570714473724
loss: 0.51202791929245
loss: 0.06321315467357635
loss: 0.277254581451416
loss: 0.3341289460659027
loss: 0.6315959095954895
loss: 0.9308861494064331
loss: 0.24432654678821564
loss: 0.9304906725883484
loss: 0.28756144642829895
loss: 0.5443951487541199
loss: 0.44527316093444824
loss: 0.297297865152359
loss: 0.16954803466796875
loss: 0.39272841811180115
loss: 0.4455038011074066
loss: 0.7685995697975159
loss: 1.3761602640151978
loss: 0.40140801668167114
loss: 0.2119186818599701
loss: 0.4489184319972992
loss: 0.27946385741233826
loss: 0.40911585092544556
loss: 0.5751164555549622
loss: 0.18605050444602966
loss: 0.0733003169298172
loss: 0.6216291785240173
loss: 0.05909409373998642
loss: 0.7733874320983887
loss: 0.5014876127243042
loss: 0.597619891166687
loss: 1.1866607666015625
loss: 0.5711327791213989
train_acc: 0.8857142857142857
test_acc: 0.8156862745098039

Epoch 17/19:
loss: 0.48713016510009766
loss: 0.47973957657814026
loss: 0.25879985094070435
loss: 0.4029591679573059
loss: 0.2519352436065674
loss: 0.3885086476802826
loss: 0.8465899229049683
loss: 0.5802454352378845
loss: 0.3619081377983093
loss: 0.5680963397026062
loss: 0.25609368085861206
loss: 0.2992027699947357
loss: 0.5492356419563293
loss: 0.0879589095711708
loss: 0.41790762543678284
loss: 0.2641911506652832
loss: 0.33924394845962524
loss: 0.11372025310993195
loss: 0.4129636883735657
loss: 0.8498841524124146
loss: 0.3836959898471832
loss: 1.5988978147506714
loss: 0.4933391213417053
loss: 0.16027812659740448
loss: 0.44848212599754333
loss: 0.8306658864021301
loss: 0.2763507664203644
loss: 0.44877713918685913
loss: 0.7196211218833923
loss: 0.13225579261779785
loss: 0.2704833447933197
loss: 0.6609905362129211
loss: 0.10600428283214569
loss: 0.4182317852973938
loss: 0.05361952632665634
loss: 0.4183807373046875
loss: 0.2976294159889221
loss: 0.02485198900103569
loss: 0.5006448030471802
loss: 0.5309582948684692
loss: 0.48586344718933105
loss: 0.6365426778793335
loss: 0.4280250668525696
loss: 0.12132787704467773
loss: 0.1730954647064209
loss: 0.5359733700752258
train_acc: 0.9032967032967033
test_acc: 0.8274509803921568

Epoch 18/19:
loss: 0.22505950927734375
loss: 0.3752087652683258
loss: 0.31798094511032104
loss: 0.10336494445800781
loss: 0.43798670172691345
loss: 0.47884926199913025
loss: 0.06612062454223633
loss: 0.2186627835035324
loss: 0.18905982375144958
loss: 0.2653622627258301
loss: 0.2539753317832947
loss: 0.17382946610450745
loss: 0.4360850751399994
loss: 0.37930265069007874
loss: 0.29689115285873413
loss: 0.1028541550040245
loss: 0.12374315410852432
loss: 0.44649070501327515
loss: 0.5049853324890137
loss: 0.1244901642203331
loss: 0.6151681542396545
loss: 0.4680864214897156
loss: 0.1588166207075119
loss: 0.09014429897069931
loss: 0.538375973701477
loss: 0.1699388474225998
loss: 0.12217569351196289
loss: 0.14235572516918182
loss: 0.17480778694152832
loss: 0.2231508493423462
loss: 1.8854265213012695
loss: 0.27104243636131287
loss: 0.21625328063964844
loss: 0.11215438693761826
loss: 1.0918924808502197
loss: 0.3065813183784485
loss: 0.21765489876270294
loss: 0.21031975746154785
loss: 0.4625961184501648
loss: 0.3581942617893219
loss: 0.4664023518562317
loss: 0.9530845880508423
loss: 0.1583324372768402
loss: 0.13401898741722107
loss: 0.05254068225622177
loss: 0.01837768591940403
train_acc: 0.9010989010989011
test_acc: 0.8392156862745098

Epoch 19/19:
loss: 0.10231252014636993
loss: 0.518302857875824
loss: 0.14299321174621582
loss: 0.320628821849823
loss: 0.17146548628807068
loss: 0.05404548719525337
loss: 0.14287595450878143
loss: 0.28975170850753784
loss: 0.3408631682395935
loss: 0.21176943182945251
loss: 0.2535528242588043
loss: 0.5441257357597351
loss: 0.07672224193811417
loss: 0.04609174653887749
loss: 0.20766958594322205
loss: 0.11915107071399689
loss: 0.23808927834033966
loss: 0.36810654401779175
loss: 0.0422704704105854
loss: 0.06852369010448456
loss: 0.08385725319385529
loss: 0.27068454027175903
loss: 0.24034623801708221
loss: 0.23787102103233337
loss: 0.25974154472351074
loss: 0.2572573125362396
loss: 0.17650394141674042
loss: 0.5591689944267273
loss: 0.3651626408100128
loss: 0.23391160368919373
loss: 0.16370491683483124
loss: 0.2896658778190613
loss: 0.518138587474823
loss: 0.3377518653869629
loss: 0.11119508743286133
loss: 0.2715741991996765
loss: 0.0004930496215820312
loss: 0.11989889293909073
loss: 0.13205118477344513
loss: 0.04471702501177788
loss: 0.1900845468044281
loss: 0.18103066086769104
loss: 0.5146373510360718
loss: 0.2089412659406662
loss: 0.05213575437664986
loss: 2.6437151432037354
train_acc: 0.9472527472527472
test_acc: 0.8666666666666667

final_test:
test_acc: 0.8666666666666667

Process finished with exit code 0